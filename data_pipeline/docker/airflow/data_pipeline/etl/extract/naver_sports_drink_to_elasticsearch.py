# -*- coding: utf-8 -*-
'''
ë„¤ì´ë²„ ê²€ìƒ‰ API í™œìš© â†’ ì´ì˜¨ìŒë£Œ ì ìœ ìœ¨ ë°ì´í„° ìˆ˜ì§‘ & Elasticsearch ë²Œí¬ ì €ì¥ (ìµœì í™” ì½”ë“œ)
'''

import urllib.request
import json
import csv
import os
import time  # âœ… ì‹¤í–‰ ì†ë„ ì¸¡ì •
from dotenv import load_dotenv
from datetime import datetime
from elasticsearch import Elasticsearch, helpers

# âœ… Elasticsearch ì„¤ì •
es = Elasticsearch(
    ["http://elasticsearch:9200"],
    basic_auth=("elastic", "qwer1234")
)
index_name = "sports_drink_search"

# âœ… í™˜ê²½ ë³€ìˆ˜ ê°€ì ¸ì˜¤ê¸°
client_id = os.getenv("NAVER_CLIENT_ID")
client_secret = os.getenv("NAVER_CLIENT_SECRET")

if not client_id or not client_secret:
    raise ValueError("API í‚¤ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. .env íŒŒì¼ì„ í™•ì¸í•˜ì„¸ìš”.")

# âœ… API URL
url = "https://openapi.naver.com/v1/datalab/search"

# âœ… í˜„ì¬ ë‚ ì§œ ê°€ì ¸ì˜¤ê¸°
today = datetime.now().strftime("%Y-%m-%d")

# âœ… ìŠ¤í¬ì¸  ìŒë£Œ í‚¤ì›Œë“œ ê·¸ë£¹
sports_drink = [
    {"groupName": "í¬ì¹´ë¦¬ìŠ¤ì›¨íŠ¸", "keywords": ["í¬ì¹´ë¦¬", "í¬ì¹´ë¦¬ìŠ¤ì›¨íŠ¸", "Pocari Sweat"]},
    {"groupName": "ê²Œí† ë ˆì´", "keywords": ["ê²Œí† ë ˆì´", "Gatorade"]},
    {"groupName": "íŒŒì›Œì—ì´ë“œ", "keywords": ["íŒŒì›Œì—ì´ë“œ", "Powerade"]},
    {"groupName": "í† ë ˆíƒ€", "keywords": ["í† ë ˆíƒ€", "Toreta"]},
    {"groupName": "ë§í‹°", "keywords": ["ë§í‹°", "Lingtea"]}
]

# âœ… ì—°ë ¹ëŒ€ë³„ ë§¤í•‘
age_group_mapping = {
    "10ëŒ€": ["2"],
    "20ëŒ€": ["3", "4"],
    "30ëŒ€": ["5", "6"],
    "40ëŒ€": ["7", "8"],
    "50ëŒ€": ["9", "10"],
    "60ëŒ€ ì´ìƒ": ["11"]
}

# âœ… ë„¤ì´ë²„ APIì—ì„œ ë°ì´í„° ìˆ˜ì§‘
def fetch_data(gender, ages):
    """ ë„¤ì´ë²„ APIì—ì„œ íŠ¹ì • ì„±ë³„ ë° ì—°ë ¹ëŒ€ì˜ ê²€ìƒ‰ ë°ì´í„°ë¥¼ ê°€ì ¸ì˜´ """
    results = {}
    for batch in [sports_drink[i:i + 5] for i in range(0, len(sports_drink), 5)]:
        body = json.dumps({
            "startDate": "2024-01-01",
            "endDate": today,
            "timeUnit": "date",
            "keywordGroups": batch,
            "device": "",
            "ages": ages,
            "gender": gender
        })

        request = urllib.request.Request(url)
        request.add_header("X-Naver-Client-Id", client_id)
        request.add_header("X-Naver-Client-Secret", client_secret)
        request.add_header("Content-Type", "application/json")

        try:
            response = urllib.request.urlopen(request, data=body.encode("utf-8"))
            if response.getcode() == 200:
                data = json.loads(response.read().decode('utf-8'))
                for group in data.get("results", []):
                    for entry in group.get("data", []):
                        period, ratio = entry["period"], entry["ratio"]
                        if period not in results:
                            results[period] = {item["groupName"]: 0 for item in sports_drink}
                        results[period][group["title"]] += ratio
        except Exception as e:
            print(f"âŒ API ìš”ì²­ ì˜¤ë¥˜: {e}")
            continue
    return results

# âœ… ë°ì´í„° ì •ê·œí™” ë° ìˆ˜ì§‘
def collect_and_normalize_data():
    """ ë„¤ì´ë²„ API ë°ì´í„°ë¥¼ ì„±ë³„ ë° ì—°ë ¹ëŒ€ë³„ë¡œ ì •ë¦¬í•˜ê³  ì •ê·œí™” """
    aggregated = {"male": {}, "female": {}}
    for group_label, age_codes in age_group_mapping.items():
        aggregated["male"][group_label] = fetch_data("m", age_codes)
        aggregated["female"][group_label] = fetch_data("f", age_codes)

    # âœ… ì ìœ ìœ¨ ì •ê·œí™” (ë¹„ìœ¨ ê³„ì‚°)
    for gender in aggregated:
        for age_group in aggregated[gender]:
            for period, group_ratios in aggregated[gender][age_group].items():
                total = sum(group_ratios.values())
                if total > 0:
                    aggregated[gender][age_group][period] = {k: round(v / total * 100, 2) for k, v in group_ratios.items()}
    
    return aggregated

# âœ… Elasticsearch ë²Œí¬ ì €ì¥ (ë‚ ì§œìˆœ ì •ë ¬)
def save_to_elasticsearch_bulk(index, aggregated_data):
    """ ìˆ˜ì§‘ëœ ë°ì´í„°ë¥¼ Elasticsearchì— ë²Œí¬ ì €ì¥ (ë‚ ì§œìˆœ ì •ë ¬) """
    actions = []
    for gender, age_groups in aggregated_data.items():
        for age_group, periods in age_groups.items():
            for period in sorted(periods.keys()):
                group_ratios = periods[period]
                for brand, ratio in group_ratios.items():
                    doc_id = f"{period}_{gender}_{age_group}_{brand}"
                    action = {
                        "_op_type": "update",
                        "_index": index,
                        "_id": doc_id,
                        "doc": {
                            "period": period,
                            "gender": gender,
                            "age_group": age_group,
                            "brand": brand,
                            "ratio": ratio,
                            "timestamp": datetime.now().isoformat()
                        },
                        "doc_as_upsert": True
                    }
                    actions.append(action)

    try:
        response = helpers.bulk(es, actions)
        print(f"âœ… Elasticsearchì— {len(actions)}ê°œ ë¬¸ì„œ ì €ì¥ ì™„ë£Œ! ğŸš€")
    except Exception as e:
        print(f"âŒ Elasticsearch ì €ì¥ ì‹¤íŒ¨: {e}")
        raise  # ì‹¤íŒ¨í•˜ë©´ DAGì´ Failed ìƒíƒœê°€ ë˜ë„ë¡ ì˜ˆì™¸ ë°œìƒ

# âœ… ì‹¤í–‰
if __name__ == "__main__":
    start_time = time.time()  # âœ… ì‹¤í–‰ ì‹œì‘ ì‹œê°„

    aggregated_data = collect_and_normalize_data()  # âœ… ë°ì´í„° ìˆ˜ì§‘ ë° ì •ê·œí™”
    save_to_elasticsearch_bulk(index_name, aggregated_data)  # âœ… Elasticsearch ì €ì¥

    elapsed_time = time.time() - start_time  # âœ… ì‹¤í–‰ ì‹œê°„ ê³„ì‚°
    print(f"\nğŸš€ ì „ì²´ ì‹¤í–‰ ì™„ë£Œ! â³ ì´ ì‹¤í–‰ ì‹œê°„: {elapsed_time:.2f}ì´ˆ")