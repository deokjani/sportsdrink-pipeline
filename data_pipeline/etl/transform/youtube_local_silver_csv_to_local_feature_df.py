import os
from datetime import datetime
from pyspark.sql import SparkSession
from dotenv import load_dotenv
from pyspark.sql.functions import lit

# âœ… 1. í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ
load_dotenv(r"C:/project/sportsdrink-pipeline-spark-airflow/data_pipeline/docker/.env")
env = os.getenv("ENV", "dev")

# âœ… 2. Spark ì„¸ì…˜ ìƒì„± + Adaptive Execution ì„¤ì •
spark = (
    SparkSession.builder
    .appName("YouTube Feature Join (Video + Channel Only)")
    .config("spark.sql.adaptive.enabled", "true")
    .config("spark.sql.adaptive.skewJoin.enabled", "true")
    .config("spark.sql.autoBroadcastJoinThreshold", 50 * 1024 * 1024)
    .getOrCreate()
)
spark.sparkContext.setLogLevel("WARN")

# âœ… 3. ê²½ë¡œ ì„¤ì •
LOAD_PATH = f"C:/project/sportsdrink-pipeline-spark-airflow/data_pipeline/data/processed/sportsdrink_youtube_search_daily_silver"
SAVE_BASE_PATH = f"C:/project/sportsdrink-pipeline-spark-airflow/data_pipeline/data/features/sportsdrink_youtube_search_daily"

# ì˜¤ëŠ˜ ë‚ ì§œ â†’ feature_date
feature_date = datetime.today().strftime("%Y-%m-%d")
save_path = os.path.join(SAVE_BASE_PATH)

# âœ… 4. CSV ë°ì´í„° ë¶ˆëŸ¬ì˜¤ê¸°
video_df = spark.read.option("header", True).csv(os.path.join(LOAD_PATH, "video_feature.csv"))
channel_df = spark.read.option("header", True).csv(os.path.join(LOAD_PATH, "channel_feature.csv"))

# âœ… 5. ì»¬ëŸ¼ëª… ë³€ê²½ (ì¶©ëŒ ë°©ì§€ìš© prefix)
rename_map_channel = {
    "channel_title": "channel_title_channel",
    "crawled_date": "crawled_date_channel",
    "feature_date": "feature_date_channel",
}
rename_map_video = {
    "channel_title": "channel_title_video",
    "crawled_date": "crawled_date_video",
    "feature_date": "feature_date_video",
    "like_count": "like_count_video"
}

for old, new in rename_map_channel.items():
    if old in channel_df.columns:
        channel_df = channel_df.withColumnRenamed(old, new)

for old, new in rename_map_video.items():
    if old in video_df.columns:
        video_df = video_df.withColumnRenamed(old, new)

# âœ… 6. ë¹„ë””ì˜¤ + ì±„ë„ ì¡°ì¸
full_feature_df = video_df.join(channel_df, on="channel_id", how="left")

# âœ… 7. ìµœì¢… Feature ì»¬ëŸ¼ ì •ì˜
selected_columns = [
    "view_count",                # ì˜ìƒ ì¡°íšŒìˆ˜
    "like_count_video",          # ì˜ìƒ ì¢‹ì•„ìš” ìˆ˜
    "like_to_view_ratio",        # ì¡°íšŒìˆ˜ ëŒ€ë¹„ ì¢‹ì•„ìš” ë¹„ìœ¨
    "comment_to_view_ratio",     # ì¡°íšŒìˆ˜ ëŒ€ë¹„ ëŒ“ê¸€ ë¹„ìœ¨
    "engagement_score",          # ì¢…í•© ì°¸ì—¬ë„ ì§€í‘œ
    "subscriber_count",          # ì±„ë„ êµ¬ë…ì ìˆ˜
    "total_views",               # ì±„ë„ ëˆ„ì  ì¡°íšŒìˆ˜
    "video_count",               # ì±„ë„ ì—…ë¡œë“œ ìˆ˜
    "is_big_channel",            # ëŒ€í˜• ì±„ë„ ì—¬ë¶€
    "channel_tier",              # ì±„ë„ ë“±ê¸‰
]
existing_columns = [col for col in selected_columns if col in full_feature_df.columns]
feature_df = full_feature_df.select(*existing_columns)

# âœ… 8. feature_date íŒŒí‹°ì…˜ ì»¬ëŸ¼ ì¶”ê°€
feature_df = feature_df.withColumn("feature_date", lit(feature_date))

# âœ… 9. ì €ì¥ (Parquet, íŒŒí‹°ì…”ë‹)
feature_df.write \
    .partitionBy("feature_date") \
    .mode("append") \
    .parquet(save_path)

print("\nâœ… í•™ìŠµìš© Feature ë°ì´í„° ì €ì¥ ì™„ë£Œ (ëŒ“ê¸€ ì œì™¸)")
print(f"ğŸ“ ì €ì¥ ìœ„ì¹˜: {save_path}/feature_date={feature_date}/")
spark.stop()
