import os
import requests
from pyspark.sql import SparkSession
from pyspark.sql.functions import col, sha2, concat_ws
from dotenv import load_dotenv

# âœ… 1. í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ
env_path = "C:\\project\\sportsdrink-pipeline-spark-airflow\\data_pipeline\\docker\\.env"
load_dotenv(env_path)

env = os.getenv("ENV", "dev")

# âœ… 2. Spark ì„¸ì…˜ ìƒì„±
spark = (
    SparkSession.builder
    .appName("Bronze Iceberg Data Quality Validation")
    .config("spark.jars.packages", ",".join([
        "org.apache.hadoop:hadoop-aws:3.3.4",
        "org.apache.iceberg:iceberg-spark-runtime-3.5_2.12:1.4.3"
    ]))
    .config("spark.sql.extensions", "org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions")
    .config("spark.sql.catalog.hadoop_hms", "org.apache.iceberg.spark.SparkCatalog")
    .config("spark.sql.catalog.hadoop_hms.catalog-impl", "org.apache.iceberg.hive.HiveCatalog")
    .config("spark.sql.catalog.hadoop_hms.uri", "thrift://localhost:9083")
    .config("spark.sql.catalog.hadoop_hms.warehouse", f"s3a://deokjin-test-datalake/data/{env}/")
    .config("spark.hadoop.fs.s3a.access.key", os.getenv("AWS_ACCESS_KEY_ID"))
    .config("spark.hadoop.fs.s3a.secret.key", os.getenv("AWS_SECRET_ACCESS_KEY"))
    .config("spark.hadoop.fs.s3a.endpoint", "s3.ap-northeast-2.amazonaws.com")
    .getOrCreate()
)
spark.sparkContext.setLogLevel("WARN")

# âœ… Slack ì•Œë¦¼ í•¨ìˆ˜
def send_quality_summary_to_slack(table_name, null_issues, dup_issues, row_issue):
    slack_url = os.getenv("SLACK_WEBHOOK_URL")
    if not slack_url:
        print("âŒ SLACK_WEBHOOK_URL í™˜ê²½ë³€ìˆ˜ ì—†ìŒ")
        return

    status = "âœ… PASS" if not (null_issues or dup_issues or row_issue) else "âŒ FAIL"

    message = {
        "attachments": [
            {
                "color": "good" if status == "âœ… PASS" else "danger",
                "title": f"ðŸ“Š ë°ì´í„° í’ˆì§ˆ ê²€ì¦ ê²°ê³¼: `{table_name}` â†’ {status}",
                "fields": [
                    {"title": "Null ì´ìŠˆ", "value": null_issues or "ì—†ìŒ", "short": True},
                    {"title": "PK ì¤‘ë³µ", "value": dup_issues or "ì—†ìŒ", "short": True},
                    {"title": "Row ìˆ˜ ê¸°ì¤€", "value": row_issue or "ì¶©ì¡±", "short": False},
                ]
            }
        ]
    }

    response = requests.post(slack_url, json=message)
    if response.status_code != 200:
        print(f"âŒ Slack ì „ì†¡ ì‹¤íŒ¨: {response.text}")
    else:
        print("âœ… Slack í’ˆì§ˆ ìš”ì•½ ì „ì†¡ ì™„ë£Œ")

# âœ… ê²€ì¦ í•¨ìˆ˜
def validate_data(df, table_name, pk_col, null_cols, min_rows=1000):
    total = df.count()

    null_issues = ""
    for col_name in null_cols:
        null_count = df.filter(col(col_name).isNull()).count()
        ratio = null_count / total if total > 0 else 0
        if ratio > 0.05:
            null_issues += f"{col_name}: {ratio:.1%} "

    dup_count = df.groupBy(pk_col).count().filter("count > 1").count()
    dup_issues = f"{dup_count}ê±´ ì¤‘ë³µ" if dup_count > 0 else ""

    row_issue = f"{total}ê±´ < {min_rows} ê¸°ì¤€" if total < min_rows else ""

    # âœ… Slack ì•Œë¦¼ ì „ì†¡
    send_quality_summary_to_slack(table_name, null_issues, dup_issues, row_issue)

# âœ… Iceberg í…Œì´ë¸” ë¶ˆëŸ¬ì˜¤ê¸°
namespace = "sportsdrink_youtube_search_daily_bronze"
channel_df = spark.read.format("iceberg").load(f"hadoop_hms.{namespace}.channel_info")
video_df = spark.read.format("iceberg").load(f"hadoop_hms.{namespace}.video_info")
comments_df = spark.read.format("iceberg").load(f"hadoop_hms.{namespace}.comments_info")

# âœ… comment_id ìƒì„± (4ê°œ ì»¬ëŸ¼ ì¡°í•© ê¸°ë°˜ ê³ ìœ  ID)
comments_df = comments_df.withColumn(
    "comment_id",
    sha2(concat_ws("||", "video_id", "author", "published_at", "comment_text"), 256)
)

# âœ… í’ˆì§ˆ ê²€ì¦ ì‹¤í–‰
validate_data(channel_df, "channel_info", "channel_id", ["channel_id", "channel_title"])
validate_data(video_df, "video_info", "video_id", ["video_id", "title"])
validate_data(comments_df, "comments_info", "comment_id", ["comment_id", "comment_text"])

print("\nðŸŽ‰ ëª¨ë“  í’ˆì§ˆ ê²€ì¦ ë° Slack ì•Œë¦¼ ì™„ë£Œ!")
spark.stop()
