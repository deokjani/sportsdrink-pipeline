# Spark Streaming Consumer: Kafka(news_rss_topic) â†’ Delta (Raw News Data)

import os
from pyspark.sql import SparkSession
from pyspark.sql.functions import from_json, col
from pyspark.sql.types import StructType, StructField, StringType
from datetime import datetime

# ì„¤ì •
SPARK_TMP_DIR = "C:/ITWILL/SportsDrinkForecast/data_pipeline/sparktmp"
DELTA_RAW_DIR = "C:/ITWILL/SportsDrinkForecast/data_pipeline/data/raw/news_delta"
KAFKA_BROKER = "localhost:29092"
NEWS_RSS_TOPIC = "news_rss_topic"

# ë‚ ì§œ ê¸°ë°˜ ì €ì¥ ë””ë ‰í† ë¦¬
today_date = datetime.now().strftime('%Y%m%d')
DELTA_RAW_SAVE_DIR = f"{DELTA_RAW_DIR}/News_RSS_Daily_{today_date}"

# Spark ì„¸ì…˜ ìƒì„±
spark = (SparkSession.builder.appName("NewsRSSKafkaToDelta")
         .config("spark.sql.extensions", "io.delta.sql.DeltaSparkSessionExtension")
         .config("spark.sql.catalog.spark_catalog", "org.apache.spark.sql.delta.catalog.DeltaCatalog")
         .config("spark.local.dir", SPARK_TMP_DIR)
         .config("spark.jars.packages",
                 "org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.0,io.delta:delta-spark_2.12:3.0.0")
         .getOrCreate())

spark.sparkContext.setLogLevel("WARN")

# RSS Kafka ë°ì´í„° ìŠ¤í‚¤ë§ˆ ì •ì˜
rss_schema = StructType([
    StructField("source", StringType(), True),
    StructField("title", StringType(), True),
    StructField("link", StringType(), True),
    StructField("summary", StringType(), True),
    StructField("published", StringType(), True)
])

# foreachBatch í•¨ìˆ˜: ë¡œê·¸ ì¶œë ¥ + Delta ì €ì¥
def write_to_delta_with_log(batch_df, batch_id):
    print(f"\nğŸ“° [Batch ID: {batch_id}] ìˆ˜ì‹ ëœ ë‰´ìŠ¤ ë°ì´í„°:")
    batch_df.show(truncate=False)

    # Delta ì €ì¥
    batch_df.write.format("delta").mode("append").save(DELTA_RAW_SAVE_DIR)

# Kafka â†’ Delta Streaming (with ë¡œê·¸)
query = (spark.readStream.format("kafka")
         .option("kafka.bootstrap.servers", KAFKA_BROKER)
         .option("subscribe", NEWS_RSS_TOPIC)
         .option("startingOffsets", "latest")
         .load()
         .selectExpr("CAST(value AS STRING) AS json_data")
         .select(from_json(col("json_data"), rss_schema).alias("data"))
         .select("data.*")
         .writeStream
         .outputMode("append")
         .option("checkpointLocation", f"{SPARK_TMP_DIR}/rss_news_checkpoint")
         .foreachBatch(write_to_delta_with_log)
         .start())

print("ğŸ“¡ Spark Streaming ì‹œì‘ë¨: Kafka(news_rss_topic) â†’ Delta ì €ì¥ ì¤‘...\n")
query.awaitTermination()
