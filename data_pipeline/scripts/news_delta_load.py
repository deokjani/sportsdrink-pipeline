# Delta Lakeì— ì €ì¥ëœ ë‰´ìŠ¤ ë°ì´í„°ë¥¼ ì¡°íšŒí•˜ëŠ” Spark ì½”ë“œ

from pyspark.sql import SparkSession
from datetime import datetime

# ê²½ë¡œ ì„¤ì • (ë‚ ì§œë³„ ë””ë ‰í† ë¦¬)
DELTA_RAW_DIR = "C:/ITWILL/SportsDrinkForecast/data_pipeline/data/raw/news_delta"
today_date = datetime.now().strftime('%Y%m%d')
DELTA_RAW_SAVE_DIR = f"{DELTA_RAW_DIR}/News_RSS_Daily_{today_date}"

# Spark ì„¸ì…˜ ìƒì„±
spark = (SparkSession.builder.appName("ReadNewsFromDelta")
         .config("spark.sql.extensions", "io.delta.sql.DeltaSparkSessionExtension")
         .config("spark.sql.catalog.spark_catalog", "org.apache.spark.sql.delta.catalog.DeltaCatalog")
         .config("spark.jars.packages", "io.delta:delta-spark_2.12:3.0.0")
         .getOrCreate())

# ë¡œê·¸ ìµœì†Œí™”
spark.sparkContext.setLogLevel("WARN")

# Delta ë°ì´í„° ì½ê¸°
df = spark.read.format("delta").load(DELTA_RAW_SAVE_DIR)

# ìƒìœ„ 10ê°œ ì¶œë ¥
print(f"ğŸ“¦ Delta ì €ì¥ì†Œì—ì„œ ë¡œë“œëœ ë‰´ìŠ¤ ë°ì´í„° (ê²½ë¡œ: {DELTA_RAW_SAVE_DIR}):")
df.show(10, truncate=False)

# í•„ìš” ì‹œ í•„í„°ë§ ì˜ˆì‹œ
# df.filter(df["source"].contains("chosun")).show()

# ì¢…ë£Œ
spark.stop()
