import os
import boto3
from pyspark.sql import SparkSession
from dotenv import load_dotenv

# âœ… í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ
env_path = "C:/project/sportsdrink-pipeline-spark-airflow/data_pipeline/docker/.env"
load_dotenv(env_path)
env = os.getenv("ENV", "dev")

# âœ… Spark ì„¸ì…˜ ìƒì„± (Hive ë©”íƒ€ìŠ¤í† ì–´ ì—°ê²°ìš©)
spark = (
    SparkSession.builder
    .appName("Drop Hive + S3 Tables")
    .config("spark.jars.packages", ",".join([
        "org.apache.hadoop:hadoop-aws:3.3.4",
        "org.apache.iceberg:iceberg-spark-runtime-3.5_2.12:1.4.3"
    ]))
    .config("spark.sql.extensions", "org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions")
    .config("spark.sql.catalog.hadoop_hms", "org.apache.iceberg.spark.SparkCatalog")
    .config("spark.sql.catalog.hadoop_hms.catalog-impl", "org.apache.iceberg.hive.HiveCatalog")
    .config("spark.sql.catalog.hadoop_hms.uri", "thrift://localhost:9083")
    .config("spark.sql.catalog.hadoop_hms.warehouse", f"s3a://deokjin-test-datalake/data/{env}/")
    .config("spark.hadoop.fs.s3a.access.key", os.getenv("AWS_ACCESS_KEY_ID"))
    .config("spark.hadoop.fs.s3a.secret.key", os.getenv("AWS_SECRET_ACCESS_KEY"))
    .config("spark.hadoop.fs.s3a.endpoint", "s3.ap-northeast-2.amazonaws.com")
    .getOrCreate()
)
spark.sparkContext.setLogLevel("WARN")

# âœ… ì‚­ì œí•  ë„¤ì„ìŠ¤í˜ì´ìŠ¤ ë° í…Œì´ë¸” ì •ì˜
namespace = "sportsdrink_youtube_search_daily_bronze"
tables = ["channel_info", "video_info", "comments_info"]

# âœ… Hive í…Œì´ë¸” ë¨¼ì € ì‚­ì œ
for table in tables:
    full_table_name = f"hadoop_hms.{namespace}.{table}"
    try:
        spark.sql(f"DROP TABLE IF EXISTS {full_table_name}")
        print(f"ğŸ§¹ Hive í…Œì´ë¸” ì‚­ì œ ì™„ë£Œ: {full_table_name}")
    except Exception as e:
        print(f"âŒ Hive í…Œì´ë¸” ì‚­ì œ ì‹¤íŒ¨: {full_table_name} | {str(e)}")

# âœ… ì´í›„ S3 ê²½ë¡œ ì‚­ì œ
try:
    s3 = boto3.resource("s3")
    bucket_name = "deokjin-test-datalake"
    prefix = f"data/{env}/{namespace}.db/"

    bucket = s3.Bucket(bucket_name)
    deleted = bucket.objects.filter(Prefix=prefix).delete()
    print(f"ğŸ§¹ S3 ê²½ë¡œ ì‚­ì œ ì™„ë£Œ: s3://{bucket_name}/{prefix}")
except Exception as e:
    print(f"âŒ S3 ì‚­ì œ ì‹¤íŒ¨: {str(e)}")

# âœ… Spark ì„¸ì…˜ ì¢…ë£Œ
spark.stop()
